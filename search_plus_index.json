{"./":{"url":"./","title":"前言","keywords":"","body":"Swift Docs 此书是openstack swift(queen)的阅读与理解，帮助大家更快了解swift的管理和使用。 GitHub地址：https://github.com/eiuapp/swift-docs 在线访问地址：https://eiu.app/swift-docs swift是著名的对象存储系统。 Welcome to Swift’s documentation! 下图是swift生态圈图： https://docs.openstack.org/swift/queens/associated_projects.html 相关资源 关于 本书中引用了一些公开的分享与链接并加以整理。 本书作于2019年初，会持续更新。 Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/env.html":{"url":"docs/env.html","title":"swift 2.17 环境配置","keywords":"","body":"这里是贯穿本书的环境 env-1 os：Ubuntu Server 16.04×64 openstack: queen 版 mariadb: 10.0 keystone: swift: 2.17.1dev20 Python: 2.7 rsync: 3.0 存储设置：4T 架构部署: 主机名 IP 作用 controller 192.168.100.50 controller node keystone 192.168.100.50 keystone node proxy 192.168.100.50 proxy object1 192.168.100.105 存储节点1(zone1) object2 192.168.100.106 存储节点2(zone1) object3 192.168.100.107 存储节点3(zone1) object4 192.168.100.107 存储节点4(zone1) 其中 192.168.100.50 有一个另一个网卡IP 192.168.0.50 env-2 os：Ubuntu Server 16.04×64 openstack: queen 版 mariadb: 10.2 keystone: swift: 2.17.1dev20 Python: 2.7 rsync: 3.0 存储设置：4T 架构部署: 主机名 IP 作用 controller 192.168.10.13 controller node mysql 192.168.10.13 存 keystone 中用户帐户密码等数据 keystone 192.168.10.13 auth Proxy 192.168.10.13 swift proxy node Proxy 192.168.10.12 swift proxy node Proxy 192.168.10.11 swift proxy node object1 192.168.0.127 存储节点1(zone1) object2 192.168.0.134 存储节点2(zone1) object3 192.168.0.135 存储节点3(zone1) object4 192.168.0.180 存储节点4(zone1) object5 192.168.0.189 存储节点5(zone1) Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/":{"url":"docs/","title":"Index","keywords":"","body":"Welcome to Swift’s documentation! updated: 2019-02-14 02:34 Welcome to Swift’s documentation Swift is a highly available, distributed, eventually consistent object/blob store. Organizations can use Swift to store lots of data efficiently, safely, and cheaply. This documentation is generated by the Sphinx toolkit and lives in the source tree. Additional documentation on Swift and other components of OpenStack can be found on the OpenStack wiki and at http://docs.openstack.org. Note If you’re looking for associated projects that enhance or use Swift, please see the Associated Projects page. Getting Started Overview and Concepts¶ Object Storage API overview Swift Architectural Overview The Rings Storage Policies The Account Reaper The Auth System Access Control Lists (ACLs) Replication Rate Limiting Large Object Support Object Versioning Global Clusters Container to Container Synchronization Expiring Object Support CORS Cross-domain Policy File Erasure Code Support Object Encryption Using Swift as Backing Store for Service Data Building a Consistent Hashing Ring Modifying Ring Partition Power Associated Projects Developer Documentation¶ Development Guidelines SAIO - Swift All In One First Contribution to Swift Adding Storage Policies to an Existing SAIO Auth Server and Middleware Middleware and Metadata Pluggable On-Disk Back-end APIs Administrator Documentation¶ Instructions for a Multiple Server Swift Installation Deployment Guide Apache Deployment Guide Administrator’s Guide Dedicated replication network Logs Swift Ops Runbook OpenStack Swift Administrator Guide Object Storage Install Guide Object Storage v1 REST API Documentation¶ See Complete Reference for the Object Storage REST API The following provides supporting information for the REST API: Object Storage API overview Discoverability Authentication Container quotas Object versioning Large objects Temporary URL middleware Form POST middleware Use Content-Encoding metadata Use the Content-Disposition metadata OpenStack End User Guide¶ The OpenStack End User Guide has additional information on using Swift. See the Manage objects and containers section. Source Documentation¶ Partitioned Consistent Hash Ring Ring Ring Builder Composite Ring Builder Proxy Proxy Controllers Proxy Server Account Account Auditor Account Backend Account Reaper Account Server Container Container Auditor Container Backend Container Server Container Reconciler Container Replicator Container Sync Container Updater Account DB and Container DB DB DB replicator Object Object Auditor Object Backend Object Replicator Object Reconstructor Object Server Object Updater Misc ACLs Buffered HTTP Constraints Container Sync Realms Direct Client Exceptions Internal Client Manager MemCacheD Request Helpers Swob Utils WSGI Storage Policy Middleware Account Quotas Bulk Operations (Delete and Archive Auto Extraction) CatchErrors CNAME Lookup Container Quotas Container Sync Middleware Cross Domain Policies Discoverability Domain Remap Dynamic Large Objects Encryption FormPost GateKeeper Healthcheck Keymaster KeystoneAuth List Endpoints Memcache Name Check (Forbidden Character Filter) Object Versioning Proxy Logging Ratelimit Recon Server Side Copy Static Large Objects StaticWeb Symlink TempAuth TempURL XProfile Indices and tables¶ Index Module Index Search Page updated: 2019-02-14 02:34 Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/getting_started.html":{"url":"docs/getting_started.html","title":"Getting Started","keywords":"","body":"Getting Started updated: 2019-02-14 02:34 Getting Started¶ System Requirements¶ Swift development currently targets Ubuntu Server 16.04, but should work on most Linux platforms. Swift开发目前针对Ubuntu Server 16.04，但应该适用于大多数Linux平台。 Swift is written in Python and has these dependencies: Python 2.7 rsync 3.0 The Python packages listed in the requirements file Testing additionally requires the test dependencies Testing requires these distribution packages Swift是用Python编写的，并具有以下依赖项： Python 2.7 rsync 3.0 The Python packages listed in the requirements file Testing additionally requires the test dependencies Testing requires these distribution packages There is no current support for Python 3. 目前没有Python 3的支持。 Development¶ To get started with development with Swift, or to just play around, the following docs will be useful: Swift All in One - Set up a VM with Swift installed Development Guidelines First Contribution to Swift Associated Projects 要开始使用Swift进行开发，或者只是玩玩，以下文档将非常有用： Swift All in One - 设置安装了Swift的VM 开发指南 对Swift的第一次贡献 相关项目 CLI client and SDK library¶ There are many clients in the ecosystem. The official CLI and SDK is python-swiftclient. Source code Python Package Index 生态系统中有许多客户端。官方CLI和SDK是python-swiftclient。 Source code Python Package Index Production¶ If you want to set up and configure Swift for a production cluster, the following doc should be useful: 如果要为生产群集设置和配置Swift，则以下文档应该很有用： Multiple Server Swift Installation 多服务器Swift安装 updated: 2019-02-14 02:34 Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/api/object_api_v1_overview.html":{"url":"docs/api/object_api_v1_overview.html","title":"Object Storage API overview","keywords":"","body":"Object Storage API overview updated: 2019-02-14 02:34 Object Storage API overview¶ OpenStack Object Storage is a highly available, distributed, eventually consistent object/blob store. You create, modify, and get objects and metadata by using the Object Storage API, which is implemented as a set of Representational State Transfer (REST) web services. OpenStack Object Storage是一个高可用，分布式，最终一致性的对象/blob存储。您可以使用Object Storage API创建，修改和获取对象和元数据，该API是作为一组Representational State Transfer（REST）Web服务实现的。 For an introduction to OpenStack Object Storage, see the OpenStack Swift Administrator Guide. 有关OpenStack对象存储的介绍，请参阅“OpenStack Swift管理员指南”。 You use the HTTPS (SSL) protocol to interact with Object Storage, and you use standard HTTP calls to perform API operations. You can also use language-specific APIs, which use the RESTful API, that make it easier for you to integrate into your applications. 您使用HTTPS（SSL）协议与对象存储进行交互，并使用标准HTTP调用来执行API操作。您还可以使用特定语言的API，这些API使用RESTful API，使您可以更轻松地集成到应用程序中。 To assert your right to access and change data in an account, you identify yourself to Object Storage by using an authentication token. To get a token, you present your credentials to an authentication service. The authentication service returns a token and the URL for the account. Depending on which authentication service that you use, the URL for the account appears in: OpenStack Identity Service. The URL is defined in the service catalog. Tempauth. The URL is provided in the X-Storage-Url response header. 要维护您有权去访问和更改帐户中的数据，您可以使用身份验证令牌(authentication token)向对象存储标识自己。要获取令牌，请将凭据提供给身份验证服务。身份验证服务返回令牌和帐户的URL。根据您使用的身份验证服务，该帐户的URL显示在： OpenStack身份服务。URL在服务目录中定义。 Tempauth。URL在X-Storage-Url响应头中提供。 In both cases, the URL is the full URL and includes the account resource. 在这两种情况下，URL都是完整的URL并包含帐户资源。 The Object Storage API supports the standard, non-serialized response format, which is the default, and both JSON and XML serialized response formats. Object Storage API支持标准的非序列化响应格式，这是默认格式，以及JSON和XML序列化响应格式。 The Object Storage system organizes data in a hierarchy, as follows: 对象存储系统按层次结构组织数据，如下所示： Account. Represents the top-level of the hierarchy. Your service provider creates your account and you own all resources in that account. The account defines a namespace for containers. A container might have the same name in two different accounts. In the OpenStack environment, account is synonymous with a project or tenant. 帐户。表示层次结构的顶级。 您的服务提供商会创建您的帐户，并拥有该帐户中的所有资源。该帐户定义容器(container)的命名空间。容器在两个不同的帐户中可能具有相同的名称。 在OpenStack环境中，帐户与项目或租户同义。 Container. Defines a namespace for objects. An object with the same name in two different containers represents two different objects. You can create any number of containers within an account. In addition to containing objects, you can also use the container to control access to objects by using an access control list (ACL). You cannot store an ACL with individual objects. In addition, you configure and control many other features, such as object versioning, at the container level. You can bulk-delete up to 10,000 containers in a single request. You can set a storage policy on a container with predefined names and definitions from your cloud provider. 容器。定义对象(object)的命名空间。两个不同容器中具有相同名称的对象表示两个不同的对象。您可以在帐户中创建任意数量的容器。 除了包含对象之外，您还可以使用容器通过使用访问控制列表（ACL）来控制对对象的访问。您不能使用单个对象存储ACL。 此外，您还可以在容器级别配置和控制许多其他功能，例如对象版本控制。 您可以在一个请求中批量删除多达10,000个容器。 您可以在具有云提供商的预定义名称和定义的容器上设置存储策略。 Object. Stores data content, such as documents, images, and so on. You can also store custom metadata with an object. With the Object Storage API, you can: Store an unlimited number of objects. Each object can be as large as 5 GB, which is the default. You can configure the maximum object size. Upload and store objects of any size with large object creation. Use cross-origin resource sharing to manage object security. Compress files using content-encoding metadata. Override browser behavior for an object using content-disposition metadata. Schedule objects for deletion. Bulk-delete up to 10,000 objects in a single request. Auto-extract archive files. Generate a URL that provides time-limited GET access to an object. Upload objects directly to the Object Storage system from a browser by using form POST middleware. Create symbolic links to other objects. 对象。存储数据内容，例如文档，图像等。您还可以使用对象存储自定义元数据。 使用Object Storage API，您可以： Store an unlimited number of objects. Each object can be as large as 5 GB, which is the default. You can configure the maximum object size. Upload and store objects of any size with large object creation. Use cross-origin resource sharing to manage object security. Compress files using content-encoding metadata. Override browser behavior for an object using content-disposition metadata. Schedule objects for deletion. Bulk-delete up to 10,000 objects in a single request. Auto-extract archive files. Generate a URL that provides time-limited GET access to an object. Upload objects directly to the Object Storage system from a browser by using form POST middleware. Create symbolic links to other objects. The account, container, and object hierarchy affects the way you interact with the Object Storage API. 帐户，容器和对象层次结构会影响您与Object Storage API交互的方式。 Specifically, the resource path reflects this structure and has this format: /v1/{account}/{container}/{object} 具体来说，资源路径反映了这种结构，并具有以下格式： /v1/{account}/{container}/{object} For example, for the flowers/rose.jpg object in the images container in the 12345678912345 account, the resource path is: /v1/12345678912345/images/flowers/rose.jpg 例如，对于帐户12345678912345中 容器 images中的flowers/rose.jpg对象，资源路径为： /v1/12345678912345/images/flowers/rose.jpg Notice that the object name contains the / character. This slash does not indicate that Object Storage has a sub-hierarchy called flowers because containers do not store objects in actual sub-folders. However, the inclusion of / or a similar convention inside object names enables you to create pseudo-hierarchical folders and directories. 请注意，对象名称包含该/字符。此斜杠不表示对象存储具有调用的子层次结构， flowers因为容器不会将对象存储在实际的子文件夹中。但是，/在对象名称中包含或类似的约定使您可以创建伪分层文件夹和目录。 For example, if the endpoint for Object Storage is objects.mycloud.com, the returned URL is https://objects.mycloud.com/v1/12345678912345. 例如，如果对象存储的端点(endpoint)是 objects.mycloud.com，则返回的URL是 https://objects.mycloud.com/v1/12345678912345。 To access a container, append the container name to the resource path. 要访问容器，请将容器名称附加到资源路径。 To access an object, append the container and the object name to the path. 要访问对象，请将容器和对象名称附加到路径。 If you have a large number of containers or objects, you can use query parameters to page through large lists of containers or objects. Use the marker, limit, and end_marker query parameters to control how many items are returned in a list and where the list starts or ends. If you want to page through in reverse order, you can use the query parameter reverse, noting that your marker and end_markers should be switched when applied to a reverse listing. I.e, for a list of objects [a, b, c, d, e] the non-reversed could be: /v1/{account}/{container}/?marker=a&end_marker=d b c 如果您有大量容器或对象，则可以使用查询参数来浏览大型容器或对象列表。使用 marker，limit和end_marker查询参数来控制多少列表中的项目，并在列表开始或结束时返回。如果要以相反的顺序翻页，可以使用查询参数reverse，注意应用于反向列表时应切换标记和end_markers,即，对于非反转的对象列表 可以是：[a, b, c, d, e]。 /v1/{account}/{container}/?marker=a&end_marker=d b c However, when reversed marker and end_marker are applied to a reversed list: /v1/{account}/{container}/?marker=d&end\\_marker=a&reverse=on c b 但是，当反向标记和end_marker应用于反转列表时： /v1/{account}/{container}/?marker=d&end\\_marker=a&reverse=on c b Object Storage HTTP requests have the following default constraints. Your service provider might use different default values. Item Maximum value Notes Number of HTTP headers 90 Length of HTTP headers 4096 bytes Length per HTTP request line 8192 bytes Length of HTTP request 5 GB Length of container names 256 bytes Cannot contain the / character. Length of object names 1024 bytes By default, there are no character restrictions. 对象存储HTTP请求具有以下默认约束。您的服务提供商可能使用不同的默认值。 项目 最大价值 笔记 HTTP标头数量 90 HTTP标头的长度 4096字节 每个HTTP请求行的长度 8192个字节 HTTP请求的长度 5 GB 容器名称的长度 256个字节 不能包含该/字符。 对象名称的长度 1024字节 默认情况下，没有字符限制。 You must UTF-8-encode and then URL-encode container and object names before you call the API binding. If you use an API binding that performs the URL-encoding for you, do not URL-encode the names before you call the API binding. Otherwise, you double-encode these names. Check the length restrictions against the URL-encoded string. 在调用API绑定之前，必须使用UTF-8编码然后对容器和对象名称进行URL编码。如果您使用为您执行URL编码的API绑定，请不要在调用API绑定之前对名称进行URL编码。否则，您将对这些名称进行双重编码。检查URL编码字符串的长度限制。 The API Reference describes the operations that you can perform with the Object Storage API: Storage accounts: Use to perform account-level tasks. Lists containers for a specified account. Creates, updates, and deletes account metadata. Shows account metadata. Storage containers: Use to perform container-level tasks. Lists objects in a specified container. Creates, shows details for, and deletes containers. Creates, updates, shows, and deletes container metadata. Storage objects: Use to perform object-level tasks. Creates, replaces, shows details for, and deletes objects. Copies objects with another object with a new or different name. Updates object metadata. API Reference描述了可以使用Object Storage API执行的操作： 存储帐户：用于执行帐户级任务。 列出指定帐户的容器。创建，更新和删除帐户元数据。显示帐户元数据。 存储容器：用于执行容器级任务。 列出指定容器中的对象。创建，显示容器的详细信息和删除容器。创建，更新，显示和删除容器元数据。 存储对象：用于执行对象级任务。 创建，替换，显示和删除对象的详细信息。使用具有新名称或不同名称的另一个对象复制对象。更新对象元数据。 updated: 2019-02-14 02:34 Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/overview_architecture.html":{"url":"docs/overview_architecture.html","title":"Swift Architectural Overview","keywords":"","body":"Swift Architectural Overview updated: 2019-02-14 02:34 Swift Architectural Overview¶ Proxy Server¶ The Proxy Server is responsible for tying together the rest of the Swift architecture. For each request, it will look up the location of the account, container, or object in the ring (see below) and route the request accordingly. For Erasure Code type policies, the Proxy Server is also responsible for encoding and decoding object data. See Erasure Code Support for complete information on Erasure Code support. The public API is also exposed through the Proxy Server. 代理服务器(Proxy Server)负责将Swift架构的其余部分捆绑在一起。对于每个请求，它将查找环中的帐户，容器或对象的位置（参见下文）并相应地路由请求。对于Erasure Code类型策略，Proxy Server还负责编码和解码对象数据。有关Erasure Code支持的完整信息，请参阅Erasure Code Support。公共API也通过代理服务器公开。 A large number of failures are also handled in the Proxy Server. For example, if a server is unavailable for an object PUT, it will ask the ring for a handoff server and route there instead. 代理服务器中也处理了大量故障。例如，如果服务器不可用于对象PUT操作，它将向环请求切换服务器并在那里路由。 When objects are streamed to or from an object server, they are streamed directly through the proxy server to or from the user – the proxy server does not spool them. 当对象流式传输到对象服务器或从对象服务器流式传出时，它们直接通过代理服务器流入或流出用户 - 代理服务器不会对它们进行假脱机（spool）。 The Ring¶ A ring represents a mapping between the names of entities stored on disk and their physical location. There are separate rings for accounts, containers, and one object ring per storage policy. When other components need to perform any operation on an object, container, or account, they need to interact with the appropriate ring to determine its location in the cluster. 环表示存储在磁盘上的实体名称与其物理位置之间的映射。每个存储策略都有一个单独的帐户，容器和对象环。当其他组件需要对 对象，容器或帐户 执行任何操作时，他们需要与相应的环进行交互以确定其在群集中的位置。 The Ring maintains this mapping using zones, devices, partitions, and replicas. Each partition in the ring is replicated, by default, 3 times across the cluster, and the locations for a partition are stored in the mapping maintained by the ring. The ring is also responsible for determining which devices are used for handoff in failure scenarios. Ring使用zones，设备，分区和副本来维护此映射。默认情况下，环中的每个分区都会在群集中复制3次（译注：复制成3份，总共3份），并且分区的位置将存储在由环维护的映射中。环还负责确定在故障情况下哪些设备用于切换。 The replicas of each partition will be isolated onto as many distinct regions, zones, servers and devices as the capacity of these failure domains allow. If there are less failure domains at a given tier than replicas of the partition assigned within a tier (e.g. a 3 replica cluster with 2 servers), or the available capacity across the failure domains within a tier are not well balanced it will not be possible to achieve both even capacity distribution (balance) as well as complete isolation of replicas across failure domains (dispersion). When this occurs the ring management tools will display a warning so that the operator can evaluate the cluster topology. 每个分区的副本将被隔离到尽可能多的不同regions，zones，服务器和设备上，因为这些故障域的容量允许（译注：这个地方翻译得拗口，其实就是“由于故障区域的允许的容量”）。如果给定层上的故障域数量少于层内分配的分区的副本数量（例如，具有2个服务器的3个副本群集），或者层内故障域的可用容量未得到很好的平衡，则无法实现均匀的容量分配（平衡）以及跨故障域（分散度dispersion）完全隔离replicas。发生这种情况时，环管理工具将显示警告，以便操作员可以评估集群拓扑。 Data is evenly distributed across the capacity available in the cluster as described by the devices weight. Weights can be used to balance the distribution of partitions on drives across the cluster. This can be useful, for example, when different sized drives are used in a cluster. Device weights can also be used when adding or removing capacity or failure domains to control how many partitions are reassigned during a rebalance to be moved as soon as replication bandwidth allows. 数据均匀分布在群集中可用的容量中，如设备权重（weight）所述。权重可用于平衡群集中驱动器上的分区分布。例如，当在群集中使用不同大小的驱动器时，这可能很有用。在添加或删除容量或故障域时，还可以使用设备权重来控制在复制带宽允许的情况下在重新平衡期间重新分配的分区数。 Note Prior to Swift 2.1.0 it was not possible to restrict partition movement by device weight when adding new failure domains, and would allow extremely unbalanced rings. The greedy dispersion algorithm is now subject to the constraints of the physical capacity in the system, but can be adjusted with-in reason via the overload option. Artificially unbalancing the partition assignment without respect to capacity can introduce unexpected full devices when a given failure domain does not physically support its share of the used capacity in the tier. 注意 在Swift 2.1.0之前，在添加新的故障域时，不可能通过设备权重限制分区移动，并且允许极不平衡的环。贪婪的分散度算法（greedy dispersion algorithm）现在受制于系统中物理容量的约束，但可以通过过载选项进行合理调整。在给定的故障域不能物理上支持其在层中使用的容量的份额时，人为地不平衡分区分配而不考虑容量会引入意外的完整设备（unexpected full devices）。 When partitions need to be moved around (for example if a device is added to the cluster), the ring ensures that a minimum number of partitions are moved at a time, and only one replica of a partition is moved at a time. 当需要移动分区时（例如，如果将设备添加到群集中），环确保一次移动最少数量的分区，并且一次只移动一个分区的一个副本。 The ring is used by the Proxy server and several background processes (like replication). See The Rings for complete information on the ring. 代理服务器和几个后台进程（如replication）使用环。有关戒指的完整信息，请参阅The Rings。 Storage Policies¶ Storage Policies provide a way for object storage providers to differentiate service levels, features and behaviors of a Swift deployment. Each Storage Policy configured in Swift is exposed to the client via an abstract name. Each device in the system is assigned to one or more Storage Policies. This is accomplished through the use of multiple object rings, where each Storage Policy has an independent object ring, which may include a subset of hardware implementing a particular differentiation. 存储策略为对象存储提供者（object storage providers）提供了一种区分Swift部署的服务级别，功能和行为的方法。Swift中配置的每个存储策略都通过抽象名称公开给客户端。系统中的每个设备都分配给一个或多个存储策略。这是通过使用多个对象环来实现的，其中每个存储策略具有独立的对象环，其可以包括实现特定区分的硬件子集。 （译注：每一个 device 都至少有一个存储策略。每一个存储策略都对应于一个对象环（不是容器环，不是账户环），但是，后文知道，这个存储策略更多的是与容器环相关联，在创建容器时，定义好存储策略，然后容器内的对象根据该定义策略来存储。） For example, one might have the default policy with 3x replication, and create a second policy which, when applied to new containers only uses 2x replication. Another might add SSDs to a set of storage nodes and create a performance tier storage policy for certain containers to have their objects stored there. Yet another might be the use of Erasure Coding to define a cold-storage tier. 例如，可以使用具有3x复制的默认策略，并创建第二个策略，当应用于新容器时，仅使用2x复制。另一个例子，可能会将SSD添加到一组存储节点，并为某些容器创建性能层存储策略，以将其对象存储在那里。另一个例子，可能是使用Erasure Coding来定义冷存储层。 This mapping is then exposed on a per-container basis, where each container can be assigned a specific storage policy when it is created, which remains in effect for the lifetime of the container. Applications require minimal awareness of storage policies to use them; once a container has been created with a specific policy, all objects stored in it will be done so in accordance with that policy. 然后，基于每个容器公开此映射，其中每个容器在创建时都可以分配特定的存储策略，该策略在容器的生命周期内保持有效。应用程序需要很少的存储策略意识才能使用它们; 一旦使用特定策略创建了容器，则存储在其中的所有对象将根据该策略完成。 The Storage Policies feature is implemented throughout the entire code base so it is an important concept in understanding Swift architecture. 存储策略功能在整个代码库中实现，因此它是理解Swift体系结构的重要概念。 See Storage Policies for complete information on storage policies. 有关存储策略的完整信息，请参阅存储策略。 Object Server¶ The Object Server is a very simple blob storage server that can store, retrieve and delete objects stored on local devices. Objects are stored as binary files on the filesystem with metadata stored in the file’s extended attributes (xattrs). This requires that the underlying filesystem choice for object servers support xattrs on files. Some filesystems, like ext3, have xattrs turned off by default. 对象服务（Object Server）是一个非常简单的blob存储服务器，可以存储，检索和删除存储在本地设备上的对象。对象作为二进制文件存储在文件系统中，元数据存储在文件的扩展属性（xattrs）中（译注：所以，当你在找到了对象后，仅通过ls -l这样的命令是看不到对象元数据的）。这要求对象服务器的基础文件系统选择支持文件上的xattrs。某些文件系统（如ext3）默认关闭xattrs。 Each object is stored using a path derived from the object name’s hash and the operation’s timestamp. Last write always wins, and ensures that the latest object version will be served. A deletion is also treated as a version of the file (a 0 byte file ending with “.ts”, which stands for tombstone). This ensures that deleted files are replicated correctly and older versions don’t magically reappear . 使用从对象名称的hash和操作的时间戳派生的路径存储每个对象。上次写入总是获胜，并确保将提供最新的对象版本（译注：这里只是确保提供最新，不是一定提供最新，这也是最终一致性的体现。如，写一个文件后，并不一定马上（比如下一秒钟），就得到这个最新的写入数据，但是，5分钟后，就一定能得到这个最新的写入数据）。删除也被视为文件的一个版本（一个以“.ts”结尾的0字节文件，代表墓碑tombstone）。这可确保正确复制已删除的文件，旧版本不会神奇地重新出现 due to failure scenarios。 Container Server¶ The Container Server’s primary job is to handle listings of objects. It doesn’t know where those object’s are, just what objects are in a specific container. The listings are stored as sqlite database files, and replicated across the cluster similar to how objects are. Statistics are also tracked that include the total number of objects, and total storage usage for that container. Container Server的主要工作是处理对象列表。它不知道那些对象在哪里，只知道特定容器中的对象。列表存储为sqlite数据库文件，并在集群中复制，类似于对象的方式。还会跟踪统计信息，其中包括对象总数以及该容器的总存储使用情况。 Account Server¶ The Account Server is very similar to the Container Server, excepting that it is responsible for listings of containers rather than objects. Account Server与Container服务非常相似，不同之处在于它负责容器而不是对象的列表。 Replication¶ Replication is designed to keep the system in a consistent state in the face of temporary error conditions like network outages or drive failures. 复制旨在使系统在面临网络中断或驱动器故障等临时错误情况时保持一致状态。 The replication processes compare local data with each remote copy to ensure they all contain the latest version. Object replication uses a hash list to quickly compare subsections of each partition, and container and account replication use a combination of hashes and shared high water marks. 复制过程将本地数据与每个远程副本进行比较，以确保它们都包含最新版本。对象复制使用哈希列表快速比较每个分区的子部分，容器和帐户复制使用哈希和共享高水位标记的组合。 Replication updates are push based. For object replication, updating is just a matter of rsyncing files to the peer. Account and container replication push missing records over HTTP or rsync whole database files. 复制更新是基于推送的。对于对象复制，更新只是将文件发送到对等方的事件。帐户和容器复制通过HTTP或rsync推送整个数据库文件丢失的记录。 The replicator also ensures that data is removed from the system. When an item (object, container, or account) is deleted, a tombstone is set as the latest version of the item. The replicator will see the tombstone and ensure that the item is removed from the entire system. 复制器(replicator)还确保从系统中删除数据。删除项目（对象，容器或帐户）时，会将逻辑删除设置为项目的最新版本。复制器将看到墓碑，并确保从整个系统中删除该项目。 See Replication for complete information on replication. 有关复制的完整信息，请参见Replication。 Reconstruction¶ The reconstructor is used by Erasure Code policies and is analogous to the replicator for Replication type policies. See Erasure Code Support for complete information on both Erasure Code support as well as the reconstructor. 重构器（reconstructor）由Erasure Code策略使用，类似于复制类型策略的复制器。有关Erasure Code支持 以及重建器的完整信息，请参阅Erasure Code Support。 Updaters¶ There are times when container or account data can not be immediately updated. This usually occurs during failure scenarios or periods of high load. If an update fails, the update is queued locally on the filesystem, and the updater will process the failed updates. This is where an eventual consistency window will most likely come in to play. For example, suppose a container server is under load and a new object is put in to the system. The object will be immediately available for reads as soon as the proxy server responds to the client with success. However, the container server did not update the object listing, and so the update would be queued for a later update. Container listings, therefore, may not immediately contain the object. 有时无法立即更新容器或帐户数据。这通常发生在故障情况或高负载期间。如果更新失败，则更新将在文件系统上本地排队，更新程序将处理失败的更新。这是最有可能进入最终一致性窗口（an eventual consistency window）的地方。例如，假设容器服务器处于负载状态，并且新对象被放入系统。一旦代理服务器成功响应客户端，该对象将立即可用于读取。但是，容器服务器未更新对象列表，因此更新将排队等待以后的更新。因此，容器列表可能不会立即包含该对象。 In practice, the consistency window is only as large as the frequency at which the updater runs and may not even be noticed as the proxy server will route listing requests to the first container server which responds. The server under load may not be the one that serves subsequent listing requests – one of the other two replicas may handle the listing. 实际上，一致性窗口仅与更新程序运行的频率一样大，甚至可能没有被注意到，因为代理服务器将列表请求路由到响应的第一个容器服务器。加载的服务器可能不是服务后续列表请求的服务器 - 另外两个副本中的一个可以处理列表。 Auditors¶ Auditors crawl the local server checking the integrity of the objects, containers, and accounts. If corruption is found (in the case of bit rot, for example), the file is quarantined, and replication will replace the bad file from another replica. If other errors are found they are logged (for example, an object’s listing can’t be found on any container server it should be). 审计员抓取本地服务器，检查对象，容器和帐户的完整性。如果发现损坏（例如，在发生损坏的情况下），则会隔离该文件，并且将从另一个副本复制替换该错误文件。如果发现其他错误，则会记录它们（例如，在任何容器服务器上都找不到对象的列表）。 updated: 2019-02-14 02:34 Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/overview_rings.html":{"url":"docs/overview_rings.html","title":"The Rings","keywords":"","body":"+++ title = \"官方中文文档 openstack swift overview rings\" date = 2019-01-21T00:00:00-08:00 lastmod = 2019-01-22T02:35:17-08:00 tags = [\"swift\", \"transilation\"] categories = [\"swift\"] draft = false weight = 3001 +++ https://docs.openstack.org/swift/queens/overview_ring.html OpenStack Docs: The Rings The Rings updated: 2019-01-15 02:30 The Rings¶ The rings determine where data should reside in the cluster. There is a separate ring for account databases, container databases, and individual object storage policies but each ring works in the same way. These rings are externally managed. The server processes themselves do not modify the rings; they are instead given new rings modified by other tools. 环确定数据应驻留在集群中的位置。帐户数据库，容器数据库和单个对象存储策略有一个单独的环，但每个环以相同的方式工作。这些环是外部管理的。服务进程本身不会修改环; 相反，它们被赋予了由其他工具修改的新环。(译注：ring不能被修改，只能产生新ring) The ring uses a configurable number of bits from the MD5 hash of an item’s path as a partition index that designates the device(s) on which that item should be stored. The number of bits kept from the hash is known as the partition power, and 2 to the partition power indicates the partition count. Partitioning the full MD5 hash ring allows the cluster components to process resources in batches. This ends up either more efficient or at least less complex than working with each item separately or the entire cluster all at once. （译注：关于下面2段文字，先看这里，可能效果要好很多。我第一次看的时候，全蒙） 环使用来自项目路径的MD5哈希的可配置位数作为分区索引（partition index），该分区索引指定应该存储该项目的设备。从散列中保留的比特数称为分区功率（partition power），2的分区功率次方表示分区总数（partition count）。对完整MD5哈希环进行分区允许群集组件批量处理资源。与单独处理每个项目或同时处理整个集群相比，这最终会更有效或至少更简单。(译注：比如initial-rings时，你配置的是swift-ring-builder account.builder create 10 3 1, 则partition power是10，partition count是1024=2**10) Another configurable value is the replica count, which indicates how many devices to assign for each partition in the ring. By having multiple devices responsible for each partition, the cluster can recover from drive or network failures. 另一个可配置的值是副本数(译注：同理，此配置副本数是3)，它指示为环中的每个分区分配的设备数量。通过让多个设备负责每个分区，群集可以从驱动器或网络故障中恢复。 Devices are added to the ring to describe the capacity available for partition replica assignments. Devices are placed into failure domains consisting of region, zone, and server. Regions can be used to describe geographical systems characterized by lower bandwidth or higher latency between machines in different regions. Many rings will consist of only a single region. Zones can be used to group devices based on physical locations, power separations, network separations, or any other attribute that would lessen multiple replicas being unavailable at the same time. 将设备添加到环中以描述可用于分区副本分配的容量。设备被置于由region, zone, 和 server组成的故障域中。Regions可用于描述以不同区域中的机器之间的较低带宽或较高等待时间为特征的地理系统。许多环只有一个region。Zones可用于根据物理位置，电源分离，网络分离或任何其他属性来对设备进行分组，这些属性可以减少同一时间多个副本都不可用的情形发生。 Devices are given a weight which describes the relative storage capacity contributed by the device in comparison to other devices. 给予设备权重，该权重描述了设备与其他设备相比所贡献的相对存储容量。 When building a ring, replicas for each partition will be assigned to devices according to the devices’ weights. Additionally, each replica of a partition will preferentially be assigned to a device whose failure domain does not already have a replica for that partition. Only a single replica of a partition may be assigned to each device - you must have at least as many devices as replicas. 构建环时，将根据设备的权重为每个分区分配副本。此外，分区的每个副本将优先分配给其故障域尚未具有该分区的副本的设备。只能为每个设备分配一个分区的单个副本 - 您必须至少拥有与副本一样多的设备。（译注：也就是设备数量最少为3） Ring Builder¶ The rings are built and managed manually by a utility called the ring-builder. The ring-builder assigns partitions to devices and writes an optimized structure to a gzipped, serialized file on disk for shipping out to the servers. The server processes check the modification time of the file occasionally and reload their in-memory copies of the ring structure as needed. Because of how the ring-builder manages changes to the ring, using a slightly older ring usually just means that for a subset of the partitions the device for one of the replicas will be incorrect, which can be easily worked around. 环由称为环构建器的实用程序手动构建和管理。环构建器将分区分配给设备，并将优化的结构的gzip压缩序列化文件写入磁盘上，以便传送到servers。服务进程偶尔检查文件的修改时间，并根据需要重新加载环结构的内存副本。由于环构建器管理环的变化，使用稍微较旧的环通常只意味着对于一个分区的子集，其中一个副本的设备将是不正确的，这可以很容易地解决。 The ring-builder also keeps a separate builder file which includes the ring information as well as additional data required to build future rings. It is very important to keep multiple backup copies of these builder files. One option is to copy the builder files out to every server while copying the ring files themselves. Another is to upload the builder files into the cluster itself. Complete loss of a builder file will mean creating a new ring from scratch, nearly all partitions will end up assigned to different devices, and therefore nearly all data stored will have to be replicated to new locations. So, recovery from a builder file loss is possible, but data will definitely be unreachable for an extended time. 环构建器还保留单独的构建器文件，其中包括环信息以及构建未来环所需的其他数据。保留这些构建器文件的多个备份副本非常重要。一种选择是将构建器文件复制到每个服务器的时候，同时复制环文件。另一种方法是将构建器文件上载到群集本身。完全丢失构建器文件意味着从头开始创建新环，几乎所有分区最终都将分配给不同的设备，因此几乎所有存储的数据都必须复制到新位置。因此，可以从构建器文件丢失中恢复，但数据肯定会在较长时间内无法访问。 Ring Data Structure¶ The ring data structure consists of three top level fields: a list of devices in the cluster, a list of lists of device ids indicating partition to device assignments, and an integer indicating the number of bits to shift an MD5 hash to calculate the partition for the hash. 环数据结构由三个顶级字段组成：集群中的设备列表，指出分区分配到设备的设备ID列表（译注：下称“分区分配列表”），以及一个整数。这个整数，用于，计算分区哈希过程中的一个MD5哈希的移位位数(译注：源码中是_part_shift，配置示例就是：22=32-10)。 （译注：根据下文知道，这三个字段名，在源码中，分别为：devs，_replica2part2dev_id，_part_shift） List of Devices¶ The list of devices is known internally to the Ring class as devs. Each item in the list of devices is a dictionary with the following keys: Ring类内部已知设备列表用devs表示。设备列表中的每个项目都是包含以下键的字典：（译注：示例可看这里 ） id integer The index into the list of devices. zone integer The zone in which the device resides. region integer The region in which the zone resides. weight float The relative weight of the device in comparison to other devices. This usually corresponds directly to the amount of disk space the device has compared to other devices. For instance a device with 1 terabyte of space might have a weight of 100.0 and another device with 2 terabytes of space might have a weight of 200.0. This weight can also be used to bring back into balance a device that has ended up with more or less data than desired over time. A good average weight of 100.0 allows flexibility in lowering the weight later if necessary. ip string The IP address or hostname of the server containing the device. port int The TCP port on which the server process listens to serve requests for the device. device string The on-disk name of the device on the server. For example: sdb1 meta string A general-use field for storing additional information for the device. This information isn’t used directly by the server processes, but can be useful in debugging. For example, the date and time of installation and hardware manufacturer could be stored here. id integer 索引到设备列表. zone integer The zone in which the device resides. region integer The region in which the zone resides. weight float 与其他设备相比，设备的相对重量。这通常直接对应于设备与其他设备相比的磁盘空间量。例如，具有1TB空间的设备可能具有100.0的权重，而具有2TB空间的另一设备可具有200.0的权重。这个权重还可以用来恢复一个设备，该设备最终会有比预期更多或更少的数据。良好的平均重量100.0可以在必要时灵活地降低重量。 ip string 包含设备的服务器的IP地址或主机名。 port int 服务器进程侦听的TCP端口为设备提供请求。 device string 服务器上设备的磁盘名称。例如：sdb1 meta string 用于存储设备的附加信息的通用字段。服务器进程不直接使用此信息，但在调试时可能很有用。例如，安装的日期和时间以及硬件制造商可以存储在此处。 Note The list of devices may contain holes, or indexes set to None, for devices that have been removed from the cluster. However, device ids are reused. Device ids are reused to avoid potentially running out of device id slots when there are available slots (from prior removal of devices). A consequence of this device id reuse is that the device id (integer value) does not necessarily correspond with the chronology of when the device was added to the ring. Also, some devices may be temporarily disabled by setting their weight to 0.0. To obtain a list of active devices (for uptime polling, for example) the Python code would look like: devices = list(self._iter_devs()) 注意 对于已从群集中删除的设备，设备列表可能包含holes或索引设置为None。但是，设备ID可以重复使用。重新使用设备ID以避免在有可用插槽（从先前移除设备）时可能耗尽设备ID插槽。此设备ID重用的结果是设备ID（整数值）不一定与设备添加到环中的时间顺序相对应。此外，可以通过将其权重设置为 0.0以暂时禁用某些设备。要获取活动设备列表（例如，用于正常运行时间轮询），Python代码将如下所示： devices = list(self._iter_devs()) Partition Assignment List¶ The partition assignment list is known internally to the Ring class as _replica2part2dev_id. This is a list of array('H')s, one for each replica. Each array('H') has a length equal to the partition count for the ring. Each integer in the array('H') is an index into the above list of devices. 分区分配列表在Ring类内部表示为 _replica2part2dev_id。这是array('H')的列表（list），每个副本一个。每个array('H')的长度等于环的分区数（译注：安装示例为：1024=2**10）。其中的array('H')上的每个整数都是上述的设备列表的索引（译注：也就是上面 List of Devices 小节中的表中的ID字段值）。 So, to create a list of device dictionaries assigned to a partition, the Python code would look like: devices = [self.devs[part2dev_id[partition]] for part2dev_id in self._replica2part2dev_id] 因此，创建分配给分区的设备列表的字典，Python代码将如下所示： devices = [self.devs[part2dev_id[partition]] for part2dev_id in self._replica2part2dev_id] array('H') is used for memory conservation as there may be millions of partitions. array('H') 用于内存保护，因为可能有数百万个分区。 Partition Shift Value¶ The partition shift value is known internally to the Ring class as _part_shift. This value is used to shift an MD5 hash of an item’s path to calculate the partition on which the data for that item should reside. Only the top four bytes of the hash are used in this process. For example, to compute the partition for the path /account/container/object, the Python code might look like: objhash = md5('/account/container/object').digest() partition = struct.unpack_from('>I', objhash)[0] >> self._part_shift 分区移位值在Ring类内部为 _part_shift。此值用于移动项目路径的MD5哈希值，以计算该项目的数据应驻留在的分区。在此过程中仅使用散列的前四个字节。例如，要计算路径的分区/account/container/object，Python代码可能如下所示： objhash = md5('/account/container/object').digest() partition = struct.unpack_from('>I', objhash)[0] >> self._part_shift For a ring generated with partition power P, the partition shift value is 32 - P. 对于使用分区功率P生成的环，分区移位值为32 - P 。 Fractional Replicas¶ A ring is not restricted to having an integer number of replicas. In order to support the gradual changing of replica counts, the ring is able to have a real number of replicas. 环不限于具有整数个副本。为了支持副本计数的逐渐变化，环能够具有实数值的副本。 When the number of replicas is not an integer, the last element of _replica2part2dev_id will have a length that is less than the partition count for the ring. This means that some partitions will have more replicas than others. For example, if a ring has 3.25 replicas, then 25% of its partitions will have four replicas, while the remaining 75% will have just three. 当副本的数量不是整数时，_replica2part2dev_id 最后一个元素的长度将小于环的分区数。这意味着某些分区将具有比其他分区更多的副本。例如，如果一个环有3.25副本，那么其25％的分区将有四个副本，而剩下的75％将只有三个副本。 （译注：3.25 = 40.25 + 30.75） Dispersion¶ With each rebalance, the ring builder calculates a dispersion metric. This is the percentage of partitions in the ring that have too many replicas within a particular failure domain. 对于每个重新平衡，环构建器计算分散度。这是环中具有特定故障域内太多副本的分区的百分比。 For example, if you have three servers in a cluster but two replicas for a partition get placed onto the same server, that partition will count towards the dispersion metric. 例如，如果群集中有三台服务器，但分区的两个副本放置在同一服务器上，则该分区将计入分散度量标准。 （译注：因为这个分区，并不分散） A lower dispersion value is better, and the value can be used to find the proper value for “overload”. 较低的分散度更好，并且该值可用于找到“overload”的适当值。 Overload¶ The ring builder tries to keep replicas as far apart as possible while still respecting device weights. When it can’t do both, the overload factor determines what happens. Each device may take some extra fraction of its desired partitions to allow for replica dispersion; once that extra fraction is exhausted, replicas will be placed closer together than is optimal for durability. 环构建器尝试尽可能远离replicas，同时仍然尊重设备weights。如果不能同时做到这两点，overload决定了会发生什么。每个设备可以占用其所需分区的一些额外部分以允许复制品分散; 一旦额外的部分耗尽，复制品将被放置在一起，而不是最佳的耐久性。 Essentially, the overload factor lets the operator trade off replica dispersion (durability) against device balance (uniform disk usage). 从本质上讲，过载因子可以让操作员在设备平衡（统一磁盘使用率）之间权衡复制分散（耐久性）。 The default overload factor is 0, so device weights will be strictly followed. 默认的过载因子是0，因此将严格遵循设备权重。 With an overload factor of 0.1, each device will accept 10% more partitions than it otherwise would, but only if needed to maintain dispersion. 在过载因子的情况下0.1，每个设备将接受比其他情况多10％的分区，但仅在需要保持分散时。 Example: Consider a 3-node cluster of machines with equal-size disks; let node A have 12 disks, node B have 12 disks, and node C have only 11 disks. Let the ring have an overload factor of 0.1 (10%). 示例：考虑具有相等大小磁盘的3节点计算机群集; 让节点A有12个磁盘，节点B有12个磁盘，节点C只有11个磁盘。让环的过载系数为0.1（10％）。 Without the overload, some partitions would end up with replicas only on nodes A and B. However, with the overload, every device is willing to accept up to 10% more partitions for the sake of dispersion. The missing disk in C means there is one disk’s worth of partitions that would like to spread across the remaining 11 disks, which gives each disk in C an extra 9.09% load. Since this is less than the 10% overload, there is one replica of each partition on each node. 如果没有过载，一些分区最终只会在节点A和B上出现副本。但是，由于过载，每个设备都愿意为了分散而接受多达10％的分区。C中丢失的磁盘意味着有一个磁盘的分区值要分布在剩余的11个磁盘上，这为C中的每个磁盘提供了额外的9.09％负载。由于这小于10％的过载，每个节点上的每个分区都有一个副本。 However, this does mean that the disks in node C will have more data on them than the disks in nodes A and B. If 80% full is the warning threshold for the cluster, node C’s disks will reach 80% full while A and B’s disks are only 72.7% full. 但是，这确实意味着节点C中的磁盘将拥有比节点A和B中的磁盘更多的数据。如果80％已满是群集的警告阈值，则节点C的磁盘将达到80％满，而A和B的磁盘只有72.7％已满。 Partition & Replica Terminology¶ All descriptions of consistent hashing describe the process of breaking the keyspace up into multiple ranges (vnodes, buckets, etc.) - many more than the number of “nodes” to which keys in the keyspace must be assigned. Swift calls these ranges partitions - they are partitions of the total keyspace. 所有关于一致性散列的描述都描述了将键空间（keyspace）分解为多个范围（vnode，buckets等）的过程 - 比必须分配键空间中的键的“节点”的数量多得多。Swift调用这些范围分区 - 它们是总键空间的分区。 Each partition will have multiple replicas. Every replica of each partition must be assigned to a device in the ring. When describing a specific replica of a partition (like when it’s assigned a device) it is described as a part-replica in that it is a specific replica of the specific partition. A single device will likely be assigned different replicas from many partitions, but it may not be assigned multiple replicas of a single partition. 每个分区都有多个副本。必须将每个分区的每个副本分配给环中的设备。在描述分区的特定副本时（例如，当它被分配设备时），它被描述为 部分副本(part-replica)，因为它是特定分区的特定副本。单个设备可能会从许多分区分配不同的副本，但可能不会为单个分区分配多个副本。 (译注：这个是肯定的。因为，创造出 partition的目的，就是为了让单个设备存放多个partition。而单个partition都有一个唯一的 partition index，同时：在xfs文件系统中，同一个文件夹下，不允许有2个同名文件夹，所以单个设备不会为单个分区分配多个副本。在上面提过的示例中，在/src/node/sdb/objects/下，不能同时有多个partiton index为119的文件夹) The total number of partitions in a ring is calculated as 2 ** . The total number of part-replicas in a ring is calculated as * 2 ** . 环中分区的总数计算为2 ** 。环中部分副本的总数计算为 * 2 ** 。（译注：安装示例，分区总数:2**10，part-replicas总数为 3*2**10) When considering a device’s weight it is useful to describe the number of part-replicas it would like to be assigned. A single device, regardless of weight, will never hold more than 2 ** part-replicas because it can not have more than one replica of any partition assigned. The number of part-replicas a device can take by weights is calculated as its parts-wanted. The true number of part-replicas assigned to a device can be compared to its parts-wanted similarly to a calculation of percentage error - this deviation in the observed result from the idealized target is called a device’s balance. 在考虑设备的权重时，描述它希望分配的部分副本的数量是有用的。无论weight如何，单个设备永远不会超过2 ** 个部分副本，因为它不能分配任何一个分区的多个副本给同一个设备。设备可以按weights计算的部分副本的数量按其parts-wanted计算。分配给设备的部分副本的真实数量可以与其parts-wanted进行比较，类似于计算百分比误差 - 观察结果与理想目标的偏差称为设备平衡（balance）。 When considering a device’s failure domain it is useful to describe the number of part-replicas it would like to be assigned. The number of part-replicas wanted in a failure domain of a tier is the sum of the part-replicas wanted in the failure domains of its sub-tier. However, collectively when the total number of part-replicas in a failure domain exceeds or is equal to 2 ** it is most obvious that it’s no longer sufficient to consider only the number of total part-replicas, but rather the fraction of each replica’s partitions. Consider for example a ring with 3 replicas and 3 servers: while dispersion requires that each server hold only ⅓ of the total part-replicas, placement is additionally constrained to require 1.0 replica of each partition per server. It would not be sufficient to satisfy dispersion if two devices on one of the servers each held a replica of a single partition, while another server held none. By considering a decimal fraction of one replica’s worth of partitions in a failure domain we can derive the total part-replicas wanted in a failure domain (1.0 * 2 ** ). Additionally we infer more about which part-replicas must go in the failure domain. Consider a ring with three replicas and two zones, each with two servers (four servers total). The three replicas worth of partitions will be assigned into two failure domains at the zone tier. Each zone must hold more than one replica of some partitions. We represent this improper fraction of a replica’s worth of partitions in decimal form as 1.5 (3.0 / 2). This tells us not only the number of total partitions (1.5 * 2 ** ) but also that each partition must have at least one replica in this failure domain (in fact 0.5 of the partitions will have 2 replicas). Within each zone the two servers will hold 0.75 of a replica’s worth of partitions - this is equal both to “the fraction of a replica’s worth of partitions assigned to each zone (1.5) divided evenly among the number of failure domains in its sub-tier (2 servers in each zone, i.e. 1.5 / 2)” but also “the total number of replicas (3.0) divided evenly among the total number of failure domains in the server tier (2 servers × 2 zones = 4, i.e. 3.0 / 4)”. It is useful to consider that each server in this ring will hold only 0.75 of a replica’s worth of partitions which tells that any server should have at most one replica of a given partition assigned. In the interests of brevity, some variable names will often refer to the concept representing the fraction of a replica’s worth of partitions in decimal form as replicanths - this is meant to invoke connotations similar to ordinal numbers as applied to fractions, but generalized to a replica instead of a four*th* or a fif*th*. The “n” was probably thrown in because of Blade Runner. 在考虑设备的故障域时，描述它希望分配的部分副本的数量是有用的。在层的故障域中需要的部分副本的数量是其子层的故障域中所需的部分副本的总和。但是，当故障域中的部分副本的总数超过或等于2 ** 时，最明显的是，仅考虑总部分副本的数量，而不是仅考虑每个副本的分区的分数，这已经不够了。例如，考虑一个具有3个副本和3个服务器的环：虽然分散度要求每个服务器仅保留总部分副本中的1/3，但是，另外，placement必须限制为 each 服务器需要1.0个副本。如果其中一个服务器上的两个设备都拥有单个分区的副本，而另一个服务器没有保留，则是不够满足分散度的。通过考虑故障域中一个副本的分区的小数部分，我们可以导出故障域中所需的总部分副本(1.0 * 2 ** )。此外，我们更多地推断出哪些部分副本必须在故障域中进行。考虑一个带有三个副本和两个zones的环，每个zones有两个服务器（总共四个服务器）。三个副本的分区将分配到zones层的两个故障域。每个zones必须包含某些分区的多于1个副本。We represent this improper fraction of a replica’s worth of partitions in decimal form as 1.5 (3.0 / 2). 这就告诉我们，不仅总分区数量（1.5 * 2 ** ），而且 each 分区在该故障域中必须至少有一个副本（其实0.5 of the partitions将有2个副本）。Within each zone the two servers will hold 0.75 of a replica’s worth of partitions - this is equal both to “the fraction of a replica’s worth of partitions assigned to each zone (1.5) divided evenly among the number of failure domains in its sub-tier (2 servers in each zone, i.e. 1.5 / 2)” but also “the total number of replicas (3.0) divided evenly among the total number of failure domains in the server tier (2 servers × 2 zones = 4, i.e. 3.0 / 4)”.考虑到此环中的每个服务器仅0.75包含0.75副本的分区，这有助于告知任何服务器最多只能分配给定分区一个副本。为了简洁起见，一些变量名称通常将表示replica的十进制形式的分区值的概念称为 replicanths - 这意味着invoke connotations，类似于应用于分数的有序数，但是副本一般化为1，而不是 a fourth or a fifth。The “n” was probably thrown in because of Blade Runner. （译注：讲真的，上面这一段，不知道在讲啥） Building the Ring¶ First the ring builder calculates the replicanths wanted at each tier in the ring’s topology based on weight. 首先，环构建器根据权重计算环的拓扑中每层所需的replicanths。(译注：也就是计算每层应该有的 replica的理论值) Then the ring builder calculates the replicanths wanted at each tier in the ring’s topology based on dispersion. 然后，环构建器根据dispersion计算环拓扑中每层所需的replicanths。 Then the ring builder calculates the maximum deviation on a single device between its weighted replicanths and wanted replicanths. 然后，环构建器计算单个设备在其weighted replicanth和wanted replicanth之间的最大偏差。 Next we interpolate between the two replicanth values (weighted & wanted) at each tier using the specified overload (up to the maximum required overload). It’s a linear interpolation, similar to solving for a point on a line between two points - we calculate the slope across the max required overload and then calculate the intersection of the line with the desired overload. This becomes the target. 接下来，我们使用指定的overload（up to the maximum required overload）在每层的两个replicanth值（weighted & wanted）之间进行插值。这是一个线性插值，类似于求解两点之间的直线上的点 - 我们计算最大所需过载的斜率，然后计算该线与所需过载的交点。This becomes the target. From the target we calculate the minimum and maximum number of replicas any partition may have in a tier. This becomes the replica-plan. 从目标，我们计算任何分区在层中可能具有的最小和最大副本数。这成为副本计划（replica-plan）。 Finally, we calculate the number of partitions that should ideally be assigned to each device based the replica-plan. 最后，我们根据副本计划计算理想情况下应分配给每个设备的分区数。 On initial balance (i.e., the first time partitions are placed to generate a ring) we must assign each replica of each partition to the device that desires the most partitions excluding any devices that already have their maximum number of replicas of that partition assigned to some parent tier of that device’s failure domain. 在初始平衡initial balance（即，第一次放置分区以生成环）时，我们必须将每个分区的每个副本分配给需要最多分区的设备，不包括已分配给某些分区的最大分区副本数的故障域的父层的设备。 When building a new ring based on an old ring, the desired number of partitions each device wants is recalculated from the current replica-plan. Next the partitions to be reassigned are gathered up. Any removed devices have all their assigned partitions unassigned and added to the gathered list. Any partition replicas that (due to the addition of new devices) can be spread out for better durability are unassigned and added to the gathered list. Any devices that have more partitions than they now desire have random partitions unassigned from them and added to the gathered list. Lastly, the gathered partitions are then reassigned to devices using a similar method as in the initial assignment described above. 在基于旧环建立新环时，从当前副本计划重新计算每个设备所需的所需分区数。接下来，将收集要重新分配的分区。任何已删除的设备都会将所有已分配的分区取消分配并添加到gathered list中。任何分区副本（由于添加了新设备）可以分散以获得更好的持久性，这些副本是未分配的并添加到收集列表中。任何具有比现在更多分区的设备，都会从它们中取消分配随机分区，并添加到收集列表中。最后，使用与上述初始分配中类似的方法将收集的分区重新分配给设备。 （译注：简单说，就是通过各种信息，收集设备，用于等会rebalance的时候分配parition） Whenever a partition has a replica reassigned, the time of the reassignment is recorded. This is taken into account when gathering partitions to reassign so that no partition is moved twice in a configurable amount of time. This configurable amount of time is known internally to the RingBuilder class as min_part_hours. This restriction is ignored for replicas of partitions on devices that have been removed, as device removal should only happens on device failure and there’s no choice but to make a reassignment. 每当分区重新分配副本时，都会记录重新分配的时间。在收集分区以重新分配时会考虑这一点，以便在可配置的时间内不会移动分区两次。这个可配置的时间量在RingBuilder类内部是 min_part_hours。对于已删除的设备上的分区副本，将忽略此限制，因为设备删除应仅在设备故障时发生，并且除了进行重新分配之外，别无选择。 The above processes don’t always perfectly rebalance a ring due to the random nature of gathering partitions for reassignment. To help reach a more balanced ring, the rebalance process is repeated a fixed number of times until the replica-plan is fulfilled or unable to be fulfilled (indicating we probably can’t get perfect balance due to too many partitions recently moved). 由于收集分区用于重新分配的随机性质，上述过程并不总是perfectly rebalance a ring。为了帮助达到更平衡的环，rebalance过程重复固定次数，直到replica-plan实现或无法实现（表明由于最近移动的分区太多，我们可能无法get perfect balance）。 Composite Rings¶ A standard ring built using the ring-builder will attempt to randomly disperse replicas or erasure-coded fragments across failure domains, but does not provide any guarantees such as placing at least one replica of every partition into each region. Composite rings are intended to provide operators with greater control over the dispersion of object replicas or fragments across a cluster, in particular when there is a desire to have strict guarantees that some replicas or fragments are placed in certain failure domains. This is particularly important for policies with duplicated erasure-coded fragments. 使用ring-builder构建的标准环将尝试跨越故障域随机分散副本或擦除编码的片段，但不提供任何保证，例如将每个分区的至少一个副本放置到每个region中。复合环（Composite rings）旨在为操作员提供对跨群集的对象replicas或fragments的分散的更大控制，特别是当希望严格保证某些复制品或片段放置在某些故障域中时。这对于具有重复的擦除编码片段的策略尤其重要。 A composite ring comprises two or more component rings that are combined to form a single ring with a replica count equal to the sum of replica counts from the component rings. The component rings are built independently, using distinct devices in distinct regions, which means that the dispersion of replicas between the components can be guaranteed. The composite_builder utilities may then be used to combine components into a composite ring. 复合环包括两个或更多个组件环，这些组件环被组合以形成单个环，其复制计数等于来自组件环的复制计数的总和。组件环是独立构建的，使用不同区域中的不同设备，这意味着可以保证组件之间的复制品的分散。composite_builder 然后可以使用这些实用程序将组件组合成复合环。 For example, consider a normal ring ring0 with replica count of 4 and devices in two regions r1 and r2. Despite the best efforts of the ring-builder, it is possible for there to be three replicas of a particular partition placed in one region and only one replica placed in the other region. For example: 例如，考虑一个正常的环ring0为4副本计数和在两个区域的设备r1和r2。尽管环构建器做了最大努力，但是有可能在一个区域中放置三个特定分区的复制品，而在另一个区域中只放置一个复制品。例如： part_n -> r1z1h110/sdb r1z2h12/sdb r1z3h13/sdb r2z1h21/sdb Now consider two normal rings each with replica count of 2: ring1 has devices in only r1; ring2 has devices in only r2. When these rings are combined into a composite ring then every partition is guaranteed to be mapped to two devices in each of r1 and r2, for example: part_n -> r1z1h10/sdb r1z2h20/sdb r2z1h21/sdb r2z2h22/sdb |_____________________| |_____________________| | | ring1 ring2 The dispersion of partition replicas across failure domains within each of the two component rings may change as they are modified and rebalanced, but the dispersion of replicas between the two regions is guaranteed by the use of a composite ring. For rings to be formed into a composite they must satisfy the following requirements: All component rings must have the same part power (and therefore number of partitions) All component rings must have an integer replica count Each region may only be used in one component ring Each device may only be used in one component ring Under the hood, the composite ring has a _replica2part2dev_id table that is the union of the tables from the component rings. Whenever the component rings are rebalanced, the composite ring must be rebuilt. There is no dynamic rebuilding of the composite ring. Note The order in which component rings are combined into a composite ring is very significant because it determines the order in which the Ring.get_part_nodes() method will provide primary nodes for the composite ring and consequently the node indexes assigned to the primary nodes. For an erasure-coded policy, inadvertent changes to the primary node indexes could result in large amounts of data movement due to fragments being moved to their new correct primary. The id of each component RingBuilder is therefore stored in metadata of the composite and used to check for the component ordering when the same composite ring is re-composed. RingBuilder ids are normally assigned when a RingBuilder instance is first saved. Older RingBuilder instances loaded from file may not have an id assigned and will need to be saved before they can be used as components of a composite ring. This can be achieved by, for example: swift-ring-builder rebalance --force Ring Builder Analyzer¶ This is a tool for analyzing how well the ring builder performs its job in a particular scenario. It is intended to help developers quantify any improvements or regressions in the ring builder; it is probably not useful to others. The ring builder analyzer takes a scenario file containing some initial parameters for a ring builder plus a certain number of rounds. In each round, some modifications are made to the builder, e.g. add a device, remove a device, change a device’s weight. Then, the builder is repeatedly rebalanced until it settles down. Data about that round is printed, and the next round begins. Scenarios are specified in JSON. Example scenario for a gradual device addition: { \"part_power\": 12, \"replicas\": 3, \"overload\": 0.1, \"random_seed\": 203488, \"rounds\": \\[ \\[ \\[\"add\", \"r1z2-10.20.30.40:6200/sda\", 8000\\], \\[\"add\", \"r1z2-10.20.30.40:6200/sdb\", 8000\\], \\[\"add\", \"r1z2-10.20.30.40:6200/sdc\", 8000\\], \\[\"add\", \"r1z2-10.20.30.40:6200/sdd\", 8000\\], \\[\"add\", \"r1z2-10.20.30.41:6200/sda\", 8000\\], \\[\"add\", \"r1z2-10.20.30.41:6200/sdb\", 8000\\], \\[\"add\", \"r1z2-10.20.30.41:6200/sdc\", 8000\\], \\[\"add\", \"r1z2-10.20.30.41:6200/sdd\", 8000\\], \\[\"add\", \"r1z2-10.20.30.43:6200/sda\", 8000\\], \\[\"add\", \"r1z2-10.20.30.43:6200/sdb\", 8000\\], \\[\"add\", \"r1z2-10.20.30.43:6200/sdc\", 8000\\], \\[\"add\", \"r1z2-10.20.30.43:6200/sdd\", 8000\\], \\[\"add\", \"r1z2-10.20.30.44:6200/sda\", 8000\\], \\[\"add\", \"r1z2-10.20.30.44:6200/sdb\", 8000\\], \\[\"add\", \"r1z2-10.20.30.44:6200/sdc\", 8000\\] \\], \\[ \\[\"add\", \"r1z2-10.20.30.44:6200/sdd\", 1000\\] \\], \\[ \\[\"set\\_weight\", 15, 2000\\] \\], \\[ \\[\"remove\", 3\\], \\[\"set\\_weight\", 15, 3000\\] \\], \\[ \\[\"set\\_weight\", 15, 4000\\] \\], \\[ \\[\"set\\_weight\", 15, 5000\\] \\], \\[ \\[\"set\\_weight\", 15, 6000\\] \\], \\[ \\[\"set\\_weight\", 15, 7000\\] \\], \\[ \\[\"set\\_weight\", 15, 8000\\] \\]\\] } History¶ The ring code went through many iterations before arriving at what it is now and while it has largely been stable, the algorithm has seen a few tweaks or perhaps even fundamentally changed as new ideas emerge. This section will try to describe the previous ideas attempted and attempt to explain why they were discarded. A “live ring” option was considered where each server could maintain its own copy of the ring and the servers would use a gossip protocol to communicate the changes they made. This was discarded as too complex and error prone to code correctly in the project timespan available. One bug could easily gossip bad data out to the entire cluster and be difficult to recover from. Having an externally managed ring simplifies the process, allows full validation of data before it’s shipped out to the servers, and guarantees each server is using a ring from the same timeline. It also means that the servers themselves aren’t spending a lot of resources maintaining rings. A couple of “ring server” options were considered. One was where all ring lookups would be done by calling a service on a separate server or set of servers, but this was discarded due to the latency involved. Another was much like the current process but where servers could submit change requests to the ring server to have a new ring built and shipped back out to the servers. This was discarded due to project time constraints and because ring changes are currently infrequent enough that manual control was sufficient. However, lack of quick automatic ring changes did mean that other components of the system had to be coded to handle devices being unavailable for a period of hours until someone could manually update the ring. The current ring process has each replica of a partition independently assigned to a device. A version of the ring that used a third of the memory was tried, where the first replica of a partition was directly assigned and the other two were determined by “walking” the ring until finding additional devices in other zones. This was discarded due to the loss of control over how many replicas for a given partition moved at once. Keeping each replica independent allows for moving only one partition replica within a given time window (except due to device failures). Using the additional memory was deemed a good trade-off for moving data around the cluster much less often. Another ring design was tried where the partition to device assignments weren’t stored in a big list in memory but instead each device was assigned a set of hashes, or anchors. The partition would be determined from the data item’s hash and the nearest device anchors would determine where the replicas should be stored. However, to get reasonable distribution of data each device had to have a lot of anchors and walking through those anchors to find replicas started to add up. In the end, the memory savings wasn’t that great and more processing power was used, so the idea was discarded. A completely non-partitioned ring was also tried but discarded as the partitioning helps many other components of the system, especially replication. Replication can be attempted and retried in a partition batch with the other replicas rather than each data item independently attempted and retried. Hashes of directory structures can be calculated and compared with other replicas to reduce directory walking and network traffic. Partitioning and independently assigning partition replicas also allowed for the best-balanced cluster. The best of the other strategies tended to give ±10% variance on device balance with devices of equal weight and ±15% with devices of varying weights. The current strategy allows us to get ±3% and ±8% respectively. Various hashing algorithms were tried. SHA offers better security, but the ring doesn’t need to be cryptographically secure and SHA is slower. Murmur was much faster, but MD5 was built-in and hash computation is a small percentage of the overall request handling time. In all, once it was decided the servers wouldn’t be maintaining the rings themselves anyway and only doing hash lookups, MD5 was chosen for its general availability, good distribution, and adequate speed. The placement algorithm has seen a number of behavioral changes for unbalanceable rings. The ring builder wants to keep replicas as far apart as possible while still respecting device weights. In most cases, the ring builder can achieve both, but sometimes they conflict. At first, the behavior was to keep the replicas far apart and ignore device weight, but that made it impossible to gradually go from one region to two, or from two to three. Then it was changed to favor device weight over dispersion, but that wasn’t so good for rings that were close to balanceable, like 3 machines with 60TB, 60TB, and 57TB of disk space; operators were expecting one replica per machine, but didn’t always get it. After that, overload was added to the ring builder so that operators could choose a balance between dispersion and device weights. In time the overload concept was improved and made more accurate. For more background on consistent hashing rings, please see Building a Consistent Hashing Ring. updated: 2019-01-15 02:30 Except where otherwise noted, this document is licensed under Creative Commons Attribution 3.0 License. See all OpenStack Legal Documents. found an error? report a bug questions? OpenStack Documentation Guides Install Guides User Guides Configuration Guides Operations and Administration Guides API Guides Contributor Guides Languages Deutsch (German) Français (French) Bahasa Indonesia (Indonesian) Italiano (Italian) 日本語 (Japanese) 한국어 (Korean) Português (Portuguese) Türkçe (Türkiye) 简体中文 (Simplified Chinese) [ swift 2.17.1.dev19 ](index.html) Getting Started Object Storage API overview Swift Architectural Overview The Rings Ring Builder Ring Data Structure Partition & Replica Terminology Building the Ring Composite Rings Ring Builder Analyzer History Storage Policies The Account Reaper The Auth System Access Control Lists (ACLs) Replication Rate Limiting Large Object Support Object Versioning Global Clusters Container to Container Synchronization Expiring Object Support CORS Cross-domain Policy File Erasure Code Support Object Encryption Using Swift as Backing Store for Service Data Building a Consistent Hashing Ring Modifying Ring Partition Power Associated Projects Development Guidelines SAIO - Swift All In One First Contribution to Swift Adding Storage Policies to an Existing SAIO Auth Server and Middleware Middleware and Metadata Pluggable On-Disk Back-end APIs Instructions for a Multiple Server Swift Installation Deployment Guide Apache Deployment Guide Administrator’s Guide Dedicated replication network Logs Swift Ops Runbook OpenStack Swift Administrator Guide Object Storage Install Guide Object Storage API overview Discoverability Authentication Container quotas Object versioning Large objects Temporary URL middleware Form POST middleware Use Content-Encoding metadata Use the Content-Disposition metadata Partitioned Consistent Hash Ring Proxy Account Container Account DB and Container DB Object Misc Middleware Page Contents The Rings Ring Builder Ring Data Structure List of Devices Partition Assignment List Partition Shift Value Fractional Replicas Dispersion Overload Partition & Replica Terminology Building the Ring Composite Rings Ring Builder Analyzer History OpenStack Projects OpenStack Security Common Questions Blog News Community User Groups Events Jobs Companies Contribute Documentation OpenStack Manuals Getting Started API Documentation Wiki Branding & Legal Logos & Guidelines Trademark Policy Privacy Policy OpenStack CLA Stay In Touch The OpenStack project is provided under the Apache 2.0 license. Openstack.org is powered by Rackspace Cloud Computing. var DOCUMENTATION_OPTIONS = { URL_ROOT: './', VERSION: '2.17.1.dev19', COLLAPSE_INDEX: false, FILE_SUFFIX: '.html', SOURCELINK_SUFFIX: '.txt', HAS_SOURCE: true }; /* build a description of this page including SHA, source location on git repo, build time and the project's launchpad bug tag. Set the HREF of the bug buttons */ var lineFeed = \"%0A\"; var gitURL = \"Source: Can't derive source file URL\"; /* there have been cases where \"pagename\" wasn't set; better check for it */ /* \"giturl\" is the URL of the source file on Git and is auto-generated by openstackdocstheme. \"pagename\" is a standard sphinx parameter containing the name of the source file, without extension. */ var sourceFile = \"overview_ring\" + \".rst\"; gitURL = \"Source: https://git.openstack.org/cgit/openstack/swift/tree/doc/source\" + \"/\" + sourceFile; /* gitsha, project and bug_tag rely on variables in conf.py */ var gitSha = \"SHA: 4ea52f3a950b5db1bb0549de322e252471218401\"; var bugProject = \"swift\"; var bugTitle = \"The Rings in swift\"; var fieldTags = \"\"; var useStoryboard = \"\"; /* \"last_updated\" is the build date and time. It relies on the conf.py variable \"html_last_updated_fmt\", which should include year/month/day as well as hours and minutes */ var buildstring = \"Release: 2.17.1.dev19 on 2019-01-15 02:30\"; var fieldComment = encodeURI(buildstring) + lineFeed + encodeURI(gitSha) + lineFeed + encodeURI(gitURL) ; logABug(bugTitle, bugProject, fieldComment, fieldTags); $(document).ready(function(){ $.ajax({ context: this, dataType : \"html\", url : \"https://docs.openstack.org/queens/badge.html\", success : function(results) { $('#deprecated-badge-container').html(results); } }); }); Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/overview_policies.html":{"url":"docs/overview_policies.html","title":"Storage Policies","keywords":"","body":"+++ title = \"官方中文文档 openstack swift overview policies\" date = 2019-01-21T00:00:00-08:00 lastmod = 2019-01-23T18:26:07-08:00 tags = [\"swift\", \"transilation\"] categories = [\"swift\"] draft = false weight = 3001 +++ https://docs.openstack.org/swift/queens/overview%5Fpolicies.html 存储策略 更新时间：2019-01-15 02:30 存储策略允许通过创建多个对象环来为各种目的对集群进行某种程度的分段。存储策略功能在整个代码库中实现，因此它是理解Swift体系结构的重要概念。 如The Rings中所述，Swift使用修改的散列环来确定数据应驻留在集群中的位置。帐户数据库，容器数据库，有一个单独的环; 每个存储策略也有一个对象环。(译者注：有3个ring)每个对象环的行为完全相同，并以相同的方式维护，但是对于策略，不同的设备可以属于不同的环。通过支持多个对象环，Swift允许应用程序和/或部署者基本上将对象存储隔离在单个集群中。有很多原因可能需要这样做： 不同级别的持久性：如果提供商想要提供（例如，2x复制和3x复制，但不想维护2个单独的群集），则他们将设置2x和3x复制策略并将节点分配给各自的环。此外，如果提供商想要提供冷存储层，他们可以创建一个擦除编码策略。 性能：正如SSD可以用作帐户或数据库环的独占成员一样，也可以创建仅限SSD的对象环，并用于实现低延迟/高性能策略。 将节点收集到组中：不同的对象环可能具有不同的物理服务器，因此特定存储策略中的对象始终位于特定的数据中心或地理位置。 不同的存储实现：另一个例子是收集使用不同Diskfile的一组节点（例如，Kinetic，GlusterFS）并使用策略将流量直接引导到那些节点。 不同的读写关联设置：可以将代理服务器配置为对每个策略使用不同的读写关联选项。有关详细信息，请参阅 每个策略配置。 注意 今天，Swift支持两种不同的策略类型：复制和擦除代码。有关详细信息，请参阅 Erasure Code Support。 另请注意，Diskfile是指后端对象存储插件架构。有关详细信息，请参阅 Pluggable On-Disk Back-end APIs。 容器和策略 策略在容器级别实施。这种方式有许多优点，其中最重要的是它能够轻松地在想要利 用它们的应用程序上make life。它还确保存储策略仍然是Swift的核心功能，与auth实现无 关。帐户/身份验证层未实施策略，是因为它需要Swift部署人员更改正在使用的所有身份验 证系统。每个容器都有一个新的特殊的不可变元数据元素，称为存储策略索引(storage policy index)。 请注意，在内部，Swift依赖于策略索引(policy indexes)而不是策略名称(policy name)。策略名称的存在是用于人类可读性，并且在代理中转换。创建容器时，支持一个新的可选header以指定策略名称。如果没有指定名称，使用默认策略（如果未定义其他策略，则将Policy-0视为默认策略）。我们将在下一节中介绍default和Policy-0之间的区别。 创建容器时会分配策略。为容器分配策略后，将无法更改该策略（除非将其删除/重新 创建）。对大型数据集的数据放置/移动的影响将使这个任务最好留给应用程序执行。 因此，如果容器具有一个存在的策略（例如3x复制），并且想要将该数据迁移到 Erasure Code策略，则应用程序将创建另一个容器，指定策略其他参数，然后只需将 数据从一个容器移动到另一个。策略适用于每个容器，允许最小的应用程序感知 (minimal application awareness); 一旦使用特定策略创建了容器，则存储在其中的 所有对象将根据该策略完成。如果删除了具有特定名称的容器（要求容器变为空），则可以创建具有相同名称的新容器，而不受先前共享相同名称的已删除容器强制执行的存储策略的任何限制。 容器与策略具有多对一关系，这意味着任意数量的容器都可以共享一个策略。可以使用特定策略的容器数量没有限制。 将环与容器相关联的概念引入了一个有趣的场景：如果同时在网络中断的任一侧使用不同的存储策略创建了2个同名容器，会发生什么？此外，如果将对象放在这些容器中，一大堆容器中，然后网络中断被恢复，会发生什么？好吧，没有特别小心，这将是一个大问题，因为应用程序最终可能会使用错误的环来尝试找到一个对象。幸运的是，这个问题有一个解决方案，一个名为Container Reconciler的守护进程不知疲倦地工作以识别和纠正这种潜在的情况。 容器协调器(Container Reconciler) 因为,无法,在分布式最终一致系统中,强制执行容器创建的原子性，所以对象写入错误 的存储策略必须最终由异步守护程序合并到正确的存储策略中。在网络分区期间从对象 写入恢复,导致了使用不同存储策略创建的脑裂容器, 由 swift-container-reconciler守护程序处理。 容器协调程序使用类似于object-expirer的队列。在容器复制期间移动队列。将容器协调器评估的对象排入队列从未被认为是错误的，因为如果对象的位置没有任何问题，则协调器将简单地将其出列。容器协调队列是对象的实际位置的索引日志，对象的实际位置发现了容器的存储策略的差异。 要确定容器的正确存储策略，当容器将状态从已删除更改为重新创建时，必须更新container_stat表中的status_changed_at字段。此事务日志允许容器复制器在复制容器和处理REPLICATE请求时更新正确的存储策略。 由于每个对象写入都是一个单独的分布式事务，因此无法根据给定容器数据库中的整个事务日志确定每个对象写入的存储策略的正确性。因此，容器数据库将始终记录对象写入，而不管基于每个对象行的存储策略。每个容器中的每个存储策略都会跟踪对象字节和计数统计信息，并使用普通对象行合并语义进行协调。 使用常规容器复制在复制期间确保对象行完全持久。在容器复制器将其对象行推送到可 用主节点之后，根据.misplaced_objects系统帐户下的对象时间戳，将任何放错位置的 对象行,批量加载到容器中。这些行,最初写入本地节点上的切换容器(handoff container)，并且在复制传递结束时，.misplaced_objects容器将复制到正确的主节点。 容器协调程序按降序处理.misplaced_objects容器，并在成功协调行所表示的对象时, 重新收集容器。容器协调程序将始终,使用通过经缓存加速后的直接容器HEAD请求,验证排队对象的正确存储策略。 由于假设，在合计下，各个存储节点的故障在大规模上是常见的，因此容器协调器,将 以简单的仲裁多数(a simple quorum majority),进行前进。在故障和rebalance的组合 期间，仲裁(a quorum)可能会提供正确存储策略的不完整记录 - 因此应用对象写入就 可能必须执行多次。由于存储节点和容器数据库,在重新应用对象写入时不会处理 X-Timestamp小于或等于其现有记录的写入，因此它们的时间戳会略微增加。为了将此 增量透明地应用于客户端，已将时间的第二个向量添加到Swift以供内部使用。见 Timestamp。 由于协调程序将对象写入应用于正确的存储策略，因此它会清除不再适用于不正确的存储策略的写入，并从.misplaced_objects容器中删除行。成功处理完所有行后，它会休眠并定期检查容器复制期间要发现的新排队行。 默认与'Policy-0' 存储策略是一种通用功能，旨在支持具有相同级别灵活性的新群集和预先存在的群集。 出于这个原因，我们引入了Policy-0, 这个与“default”策略不同的概念。正如您将在我 们开始配置策略时所看到的，每个策略都有一个名称和任意数量的别名（人性化的，可 配置的）以及索引（或简单策略编号）。Swift保留索引0以映射到所有安装中存在的对象环（例如，/etc/swift/object.ring.gz）。您可以将此策略命名为您喜欢的任何名称，如果未定义任何策略，则会将其自身报告为Policy-0，但是您无法更改索引，因为必须始终存在索引为0的策略。 另一个重要概念是默认策略，它可以是群集中的任何策略。默认策略是在未指定存储策 略的情况下发送容器创建请求时自动选择的策略。 Configuring Policies介绍了如何设置默认策略。区别Policy-0是微妙的，但非常重要。 Policy-0是Swift在访问预存储策略容器没有策略时使用的 - 在这种情况下，我们不会使用默认值，因为它可能没有与旧容器相同的策略。如果未定义其他策略，Swift将始终选择Policy-0默认值。 换句话说，默认策略意味着“如果没有指定其他内容则使用此策略创建”，并且Policy-0 策略意味着“如果没有容器策略则使用旧策略”,这实际上意味着使用object.ring.gz去查找。 注意 使用基于存储策略的代码，无法创建没有策略的容器。如果没有提供任何内容，Swift 仍将选择默认策略并将其分配给容器。对于在引入存储策略之前创建的容器，将使用旧的Policy-0。 弃用策略 有时候不再需要一种策略; 但是，简单地删除策略和相关的环对于现有数据将是有问题的。为了确保资源不会在群集中孤立（留在磁盘上但不再可访问），并且在需要停用策略时向应用程序提供正确的消息传递，则使用弃用(deprecation)的概念。Configuring Policies描述了如何弃用策略。 Swift对弃用策略的行为如下： 已弃用的策略不会出现在/info中 在使用已弃用的策略创建的预先存在的容器上仍允许PUT/GET/DELETE/POST/HEAD 尝试使用已弃用的策略创建新容器时，客户端将收到“400 Bad Request”错误 客户端仍然可以通过HEAD访问预先存在的容器上的策略统计信息 注意 策略不能同时是默认策略和弃用策略。如果您弃用默认策略，则必须指定新的默认策略。 您还可以使用已弃用的功能部署新策略。如果想在使得新存储策略普遍可用之前测试新 存储策略，则可以在最初将新策略推送到所有节点时弃用该策略。被弃用将使其天生 (render it innate)并且无法使用。要测试它，您需要使用该存储策略创建容器; 这将 需要单个proxy实例（或一组仅在内部可访问的proxy-server），这些proxy实例已经被 一次性配置, 这个配置是带有新策略且未标记为已弃用。使用新存储策略创建容器后， 任何授权使用该容器的客户端都将能够在新存储策略中添加和访问存储在该容器中的数 据。如果满意，你可以推出一个新的swift.conf,它不再将策略标记为已弃用,到所有节点。 配置策略 注意 有关向 SAIO设置添加策略的分步指南，请参阅 Adding Storage Policies to an Existing SAIO。 部署者必须充分了解配置策略的语义。配置策略分为三个步骤： 编辑/etc/swift/swift.conf文件以定义新策略。 创建相应的策略对象环文件。 （可选）创建特定于策略的proxy-server配置设置。 定义策略 每个策略都由/etc/swift/swift.conf文件中的一个section定义。节名称必须是 [storage-policy:]的格式, 所在是策略索引。除了可读性之外，没有理由将策略索引按顺序排列，但强制执行以下规则： 如果一个策略具有索引0,且索引0且未定义其他策略，则Swift将创建一个使用索引0的默认策略。 策略索引必须是非负整数。 策略索引必须是唯一的。 警告 创建和使用策略后，永远不应更改策略索引。更改策略索引可能会导致无法访问数据。 每个策略section包含以下选项： name = （必需） 策略的主要名称。 策略名称不区分大小写。 策略名称必须仅包含字母，数字或短划线。 策略名称必须是唯一的。 策略名称可以更改。 名称Policy-0只能用于具有索引0的策略。 aliases = [, , ...] （可选的） 以逗号分隔的策略备用名称列表。 默认值是空列表（即没有别名）。 所有别名必须遵循该name选项的规则。 别名可以添加到列表中或从列表中删除。 如果更改主名称，别名可用于保留对旧主名称的支持。 default = [true|false] （可选的） 如果true在客户端未指定策略时将使用此策略。 默认值为false。 通过在所需的策略部分中进行设置，可以随时更改默认策略 。default = true 如果没有将策略声明为缺省策略且未定义其他策略，则将具有索引的策略0设置为缺省策略; 否则，必须将一个策略声明为default。 不能将不推荐使用的策略声明为默认值。 有关详细信息，请参阅默认与'Policy-0'。 deprecated = [true|false] （可选的） 如果true此时无法使用此策略创建新容器。 默认值为false。 可以通过将deprecated选项添加到所需的策略部分来弃用任何策略。但是，不推荐使用已弃用的策略作为默认值。因此，由于必须始终存在默认策略，因此必须始终至少有一个未弃用的策略。 有关详细信息，请参阅 Deprecating Policies。 policy_type = [replication|erasure_coding] （可选的） 该选项policy_type用于区分不同的策略类型。 默认值为replication。 定义EC策略时使用该值erasure_coding。 EC策略类型具有其他必需选项。有关详细信息，请参阅 Using an Erasure Code Policy。 以下是正确配置的swift.conf文件的示例。有关使用此示例配置而去设置一个all-in-one的完整说明，请参阅 Adding Storage Policies to an Existing SAIO： [swift-hash] # random unique strings that can never change (DO NOT LOSE) # Use only printable chars (python -c \"import string; print(string.printable)\") swift_hash_path_prefix = changeme swift_hash_path_suffix = changeme [storage-policy:0] name = gold aliases = yellow, orange policy_type = replication default = yes [storage-policy:1] name = silver policy_type = replication deprecated = yes 创建一个环 一旦swift.conf配置为一个新的策略，必须创建一个新的环。环工具不支持策略名称， 因此在创建新策略的环文件时使用正确的策略索引至关重要。使用 swift-ring-builder创建其他对象环，与传统环是相同的方式,除了在构建文件名称的 单词object后面附加-N，其中N匹配在swift.conf中使用的策略索引。因此，如果是要为索引1策略创建环： swift-ring-builder object-1.builder create 10 3 1 在swift-ring-builder用于添加设备，rebalance等操作时，继续使用相同的命名约定。 此命名约定也用于pre-policy 存储节点数据字典的模式。 注意 确实可以将相同的驱动器用于多个策略，并且将在后面的部分中介绍如何在磁盘上管理它的详细信息，在设置之前了解此类配置的含义非常重要。确保它真的是你想要做的，在许多情况下它会是，但在其他情况下可能不是。 Proxy server 配置（可选） 与读取和写入关联相关的 Proxy Server配置选项，可以用单个存储策略来选择性覆盖。 有关详细信息，请参阅 Per policy configuration。 使用策略 使用策略非常简单 - 仅在最初创建容器时指定策略。没有其他API可以更改策略。创建容器可以在没有任何特殊策略信息的情况下完成： curl -v -X PUT -H 'X-Auth-Token: ' \\ http://127.0.0.1:8080/v1/AUTH_test/myCont0 假设我们使用上面的swift.conf示例，这将导致创建与策略名称“gold”关联的容器。它会使用'gold'，因为它被指定为默认值。现在，当我们将一个对象放入这个容器时，它将放置在我们创建的为策略'gold'环的一部分的节点上。 如果我们想明确声明我们想要策略'gold'，那么命令只需要包含一个新的头，如下所示： curl -v -X PUT -H 'X-Auth-Token: ' \\ -H 'X-Storage-Policy: gold' http://127.0.0.1:8080/v1/AUTH_test/myCont0 就是这样！应用程序不需要再次指定策略名称。然而，有一些非法操作： 如果指定了无效（错字，不存在）策略：400 Bad Request 如果您尝试通过PUT或POST更改策略：409 Conflict 如果您想了解如何使用群集中的存储，只需将HEAD the account即可，您不仅可以看到累积 数量，也可以像之前的一样，看到每个策略统计数据。在下面的示例中，共有3个对象，其中两个在策略'gold'中，另一个在策略'silver'中： curl -i -X HEAD -H 'X-Auth-Token: ' \\ http://127.0.0.1:8080/v1/AUTH_test 并且您的结果将包括（为了便于阅读，删除了一些输出）： X-Account-Container-Count: 3 X-Account-Object-Count: 3 X-Account-Bytes-Used: 21 X-Storage-Policy-Gold-Object-Count: 2 X-Storage-Policy-Gold-Bytes-Used: 14 X-Storage-Policy-Silver-Object-Count: 1 X-Storage-Policy-Silver-Bytes-Used: 7 在引擎盖下 现在我们已经解释了一些关于策略是什么以及如何配置/使用它们，让我们来探讨存储 策略如何适用于螺母级别（the nut-n-bolts level）。 解析和配置 模块Storage Policy负责解析 swift.conf文件，验证输入以及通过类 StoragePolicyCollection 创建已配置策略的全局集合(a global collection)。该集 合由类StoragePolicy策略组成。集合类包括通过名称或索引获取策略，获取有关策略 等信息的便捷功能。还有一个非常重要的function，get_object_ring()。对象环是类 StoragePolicy的成员，实际上在load_ring() 方法被调用之前不会实例化。Any caller anywhere in the code base that needs to access an object ring must use the POLICIES global singleton to access the get_object_ring() function and provide the policy index which will call load_ring() if needed; however, when starting request handling services such as the Proxy Server rings are proactively loaded to provide moderate protection against a mis-configuration resulting in a run time error. Swift启动时会实例化全局，并提供修补测试代码的策略的机制。 中间件 中间件可以通过POLICIES全局变量利用策略，并通过导入get_container_info()来访问 与相关容器关联的策略索引。通过策略索引，它可以使用 POLICIES singleton来获取正确的环。例如，List Endpoints使用刚刚描述的方法识别策略。另一个例子是Recon，它将报告所有环的md5总和。 Proxy Server 该 Proxy Server模块在存储策略的任务主要是确保正确的ring作为其成员的元素。在策 略之前，一个对象环将在实例化Application类时被实例化，并且可以通过init参数被测 试代码覆盖。但是，对于策略，没有init参数，而Application类依赖于POLICIES global singleton 来检索,在第一次需要时实例化的,环。因此，替代Application该类的对象环成员，有一个访问器函数，get_object_ring()从 POLICIES中获取环。 通常，当代理上运行的任何模块需要对象环时，它首先从缓存的容器信息中获取策略索引。 例外是在容器创建期间，它使用请求头中的策略名称从POLICIES全局变量查找策略索引。 一旦代理确定了策略索引，它就可以使用前面描述的get_object_ring()方法来访问正确 的环。然后，它有责任通过header X -Backend-Storage-Policy-Index将索引信息而不是 策略名称传递给后端服务器。走另一条路，proxy还会从返回到客户端的标头中删除索引，并确保它们只能看到友好的策略名称。 On Disk Storage 每个策略在后端服务器上都有自己的目录，并由其存储策略索引标识。按策略索引组织后 端目录结构有助于跟踪事物，并允许在策略之间共享磁盘，这可能是有意义的也可能没有意义，具体取决于提供者的需求。稍后会详细介绍，但现在请注意以下目录命名约定： /objects 映射到与Policy-0关联的对象 /objects-N 映射到存储策略索引#N /async_pending 映射到Policy-0的异步挂起更新 /async_pending-N 映射到存储策略索引#N的异步挂起更新 /tmp 映射到Policy-0的DiskFile临时目录 /tmp-N 映射到策略索引#N的DiskFile临时目录 /quarantined/objects 映射到Policy-0的隔离目录 /quarantined/objects-N 映射到策略索引#N的隔离目录 请注意，这些目录名实际上由特定的Diskfile实现拥有，上面显示的名称由默认的Diskfile使用。 对象服务 Object Server不参与直接选择存储策略。但是，由于如何为策略设置后端目录结构， 如前所述，对象服务模块确实起作用。当对象服务获取Diskfile时，它会传入策略索 引并保留实际的目录命名/结构机制给Diskfile。通过传入索引，Diskfile正在使用的实例 将确保数据根据其策略正确定位在树中。 出于同样的原因， Object Updater也具有策略意识。如前所述，不同的策略使用不同的异步挂起目录 (async pending directories)，因此更新程序需要知道如何正确扫描它们。 Object Replicator是策略意识到的是，根据策略，可能需要做大幅度不同的东西，或 者也许不是。例如，处理2x与3x的复制作业的差异是微不足道的;但是，3x和纠删码之 间处理复制的差异绝对不是微不足道的。实际上，术语“replication”实际上并不适合 某些策略，如纠删码;但是，收集和处理工作的大多数框架都很常见。因此，复制器中 的这些功能被用于所有策略，然后对每个策略都需要特定于策略的代码，并在需要时定义策略时添加。 由于同样的原因，ssync功能具有策略感知功能。某些其他模块可能不会受到明显影响， 但Diskfile所拥有的后端目录结构需要策略索引参数。因此，ssync被策略意识,真的 意味着,传递策略索引。见关于SSYNC的ssync_sender和 ssync_receiver更多信息。 就Diskfile其本身而言，策略感知就是使用提供的策略索引来管理后端结构。换句话 说，获取Diskfile实例的调用者提供策略索引，并且Diskfile工作就是通过此索引 （however it chooses）保持数据分离，以便策略可以共享相同的媒体/节点（如果需 要）。Diskfile包含实现,列出了前面描述的目录结构，但是它在内部Diskfile; 外部 模块无法了解该细节。提供了一种通用功能，用于根据策略索引映射各种目录名称和/ 或字符串。例如，Diskfile定义get_data_dir(),它建立在一个通用的get_policy_string()之上,以便为各种用法一致地构建策略感知字符串。 Container Server Container Server起着存储策略非常重要的作用，它负责处理在容器中策略的指令，预 防坏事,比如改变策略或选择了错误的策略,在没有其它参数或事物被指定的时候（回忆前面讨论的Policy-0与default）。 Container Updater是策略感知的，但是它的工作是非常简单的，通过请求头传相应于 Account Server策略索引。 Container Backend同时负责修改现有的DB schema，以及确保新的DB创建schema是支持存储策略的。容器模式的“按需”迁移允许Swift在没有停机的情况下进行升级（无论行数如何，sqlite的alter语句都很快）。为了支持滚动升级（和降级），对container_stat表的不兼容模式更改将对表进行 container_info，并且该container_stat表将替换为包含INSTEAD OF UPDATE触发器的视图，该触发器使其行为类似于旧表。 策略索引存储在此处用于报告有关容器的信息以及管理裂脑情景引起的容器及其存储策略之间的差异。此外，在裂脑期间，必须准备容器以跟踪来自多个策略的对象更新，因此对象表还包括 storage_policy_index列。Per-policy对象计数和字节在policy_stat表中使用INSERT和DELETE触发器更新，类似于container_stat直接更新的pre-policy触发器。 当Container更新container_info表中的reconciler_sync_point条目时，Container Replicator守护程序将主动迁移旧模式，作为其正常一致性检查过程的一部分 。这确 保了不会遇到任何写入的读取繁重的容器仍将迁移到与存储后策略查询完全兼容，而不 必回退,不必使用旧模式重试查询以向服务容器读取请求。 Container Sync功能，仅需要策略感知的，因为它访问对象环。因此，它需要从容器信 息中提取策略索引，并使用它从POLICIES global 中选择适当的对象环。 Account Server Account Server在存储策略的作用实在有限。当对account发出HEAD请求时（请参阅前面提供的示例），帐户服务器提供存储策略索引，并基于每个策略为客户端构建object_count和byte_count信息。 由于某些特定于策略的数据库架构更改，account server能够报告每个存储策略对象和 字节数。特定策略的policy_stat表能基于每个策略的信息维护信息（每个策略一行）, 以与 account_stat表相同的方式。该account_stat表仍然用于相同的目的而不会被 policy_stat替换，它保存总帐户统计数据，而policy_stat只是分解数据。后端还负责 通过更改DB schema来迁移pre-storage-policy accounts，并在该时间点, 使用当前 account_stat数据去填充Policy-0 的policy_stat表。 pre-storage-policy对象和字节数不会随每个对象PUT和DELETE请求一起更新，而是对 account server的容器更新由swift-container-updater异步执行。 升级和确认功能 升级到具有存储策略支持的Swift版本并不困难，事实上，集群管理员不需要进行任何特 殊的配置更改即可开始。Swift将自动开始使用现有对象环作为default环和Policy-0环。 添加policy 0的声明是完全可选的，在不存在时，给予隐式策略0的名称将是“Policy-0”。让我们出于测试目的，您希望获取已经拥有大量数据的现有集群，并使用存储策略升级到Swift。从那里你想要继续创建一个策略并测试一些东西。你需要做的就是： 将所有Swift节点升级到策略感知版本的Swift 在/etc/swift/swift.conf中定义您的策略 创建相应的对象环 创建容器和对象，并按预期确认其放置位置 有关引导您完成这些步骤的特定示例，请参阅Adding Storage Policies to an Existing SAIO。 注意 如果从已启用存储策略的Swift版本降级到不支持策略的旧版本，您将无法访问存储 在索引为0的策略之外的任何数据，但这些对象将显示在容器列表中（如果有网络分 区和un-reconciled对象，可能会重复）。在启用其他存储策略以确保为客户提供一 致的API体验之前，对升级的部署执行任何必要的集成测试非常重要。一旦暴露多个存储策略，请勿降级到不支持存储策略的Swift版本。 Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/overview_reaper.html":{"url":"docs/overview_reaper.html","title":"The Account Reaper","keywords":"","body":"+++ title = \"官方中文文档 openstack swift overview reaper\" date = 2019-01-21T00:00:00-08:00 lastmod = 2019-01-23T23:20:40-08:00 tags = [\"swift\", \"transilation\"] categories = [\"swift\"] draft = false weight = 3001 +++ https://docs.openstack.org/swift/queens/overview%5Freaper.html 账户收割者 更新时间：2019-01-15 02:30 帐户收割者在后台删除已删除帐户中的数据。 如果reseller在帐户的存储URL上发出DELETE请求，则会将帐户标记为删除。这只是将 DELETED值放入帐户数据库（和副本）account_stat表的状态列中，说明稍后应删除该帐户的数据。 通常没有设定的保留时间且没有取消删除; 假设reseller将实施此类功能，并且只有在真正希望删除帐户的数据时才在帐户上调用DELETE。但是，为了保护Swift群集帐户免受不正确或错误删除请求的影响，您可以在account-server.conf的[account-reaper]部分设置delay_reaping值，以延迟实际删除数据。目前，取消删除帐户没有任何效用; 必须直接更新帐户数据库副本，将status列设置为空字符串并将put_timestamp更新为大于delete_timestamp。（在TODO列表上正在编写一个实用程序来执行此任务，最好是通过REST调用。） 帐户收割机在每个帐户服务器上运行，并偶尔扫描服务器以查找标记为删除的帐户数据库。它只会在服务器是主节点的帐户上触发，因此多个帐户服务器并非都在尝试同时执行相同的工作。使用多个服务器删除一个帐户可能会提高删除速度，但需要协调，因此不会重复工作。速度确实不是数据删除所关注的问题，并且通常不会删除大型帐户。 帐户本身的删除过程非常简单。对于帐户中的每个容器，将删除每个对象，然后删除容器。 任何失败的删除请求都不会停止整个过程，但会导致整个过程最终失败（例如，如果对象删 除超时，则以后无法删除容器，因此帐户也无法删除）。整个过程即使在发生故障时也会继 续，因此不会因为一个麻烦的地点而无法收回集群空间。帐户收割者将继续尝试删除帐户， 直到它最终变为空，此时db_replicator中的数据库回收进程最终将删除数据库文件。 有时，持久性错误状态可能会阻止某些对象或容器被删除。如果发生这种情况，您将在日志中看到诸如“Account has not been reaped since ”的消息。您可以使用account-server.conf文件的[account-reaper]部分中的reap_warn_after值来控制何时记录此值。默认情况下，是30天。 历史 首先，考虑通过完全外部调用删除帐户的简单方法，因为它不需要对系统进行任何更改。所有数据都将通过公共REST API以与实际用户相同的方式删除。但是，缺点是它会使用代理资源并在不需要时记录所有内容。此外，它可能需要一个或两个专用服务器，仅用于发出删除请求。 还考虑了一种完全自下而上的方法，其中对象和容器服务器偶尔会扫描它们所持有的数据并 检查帐户是否被删除，如果是，则删除数据。好处是回收的速度，对代理或日志记录没有影 响，但缺点是几乎100％的扫描都会导致无缘无故地创建大量I/O负载。 还考虑了更多以容器服务器为中心的方法，其中帐户服务器将所有容器标记为删除，容器服 务器将删除每个容器中的对象然后删除自己。这对于拥有大量容器的帐户仍然可以快速回收， 但具有相当大的负载峰值的缺点。可以减慢这个过程以减轻负载峰值的可能性，但是就会失 去快速回收的好处，而剩下的只是一个更复杂的过程。此外，扫描所有容器以查找标记为删 除的容器，而大多数容器似乎不会浪费。db_replicator可以在执行复制扫描时执行此操作， 但它必须生成并跟踪看起来不必要复杂的删除过程。 最后，如上所述，以帐户服务器为中心的方法似乎是最好的。 Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "},"docs/tech_resource.html":{"url":"docs/tech_resource.html","title":"swift技术工具与资源","keywords":"","body":"swift技术工具与资源 厂商 swiftstack https://github.com/swiftstack/ 监控工具 Tulsi https://github.com/vedgithub/tulsi swift-drive-audit 磁盘监控工具 swift-exporter https://github.com/swiftstack/swift-exporter 基准测试工具 ssbench https://github.com/swiftstack/ssbench cosbench https://github.com/intel-cloud/cosbench Openstack Swift实践指南 | Openstack Swift实践指南点击关注【servicemesher】公众号回复【加群】加入学习群 | Copyright © eiu.app 2017-2019 all right reserved，powered by Gitbook Updated at 2019-02-19 14:58:02 "}}